# ü§ñ DevRAG Local - Assistant Technique Priv√©

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-v0.3-green)
![Ollama](https://img.shields.io/badge/Ollama-Local%20AI-orange)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)

**DevRAG Local** est un syst√®me de **RAG (Retrieval-Augmented Generation)** con√ßu pour les d√©veloppeurs exigeants. Il permet de discuter avec une base documentaire technique (PDFs et Sites Web) en utilisant des mod√®les d'IA tournant enti√®rement sur votre machine.

üîí **Confidentialit√© garantie :** Vos documents et vos questions ne quittent jamais votre ordinateur.

---

## üèóÔ∏è Architecture Technique

Le projet suit une architecture **ETL (Extract, Transform, Load)** modulaire :

1.  **Ingestion (`ingestion.py`)** :
    * **Scraping Intelligent :** Navigation r√©cursive avec filtrage strict (Regex) pour rester dans le p√©rim√®tre (ex: ne pas sortir de `/docs/tutorial/`).
    * **Mode Turbo :** T√©l√©chargement multi-thread√© pour une rapidit√© maximale.
    * **Nettoyage Avanc√© :** D√©tection automatique des langages de programmation et formatage en blocs Markdown pour le LLM.
    * **Historique :** Syst√®me de cache (`scraping_history.json`) pour ne jamais t√©l√©charger deux fois la m√™me page.

2.  **Vectorisation (`build_db.py`)** :
    * **Splitter :** D√©coupage du texte optimis√© pour le code (pr√©serve les classes et fonctions).
    * **Embeddings :** Utilisation de `nomic-embed-text` via Ollama.
    * **Stockage :** Persistance dans **ChromaDB** (base vectorielle locale).

3.  **Inf√©rence (`main.py` / `interface.py`)**:
    * **Retriever :** Recherche MMR (Maximum Marginal Relevance) pour diversifier les sources.
    * **LLM :** G√©n√©ration de r√©ponse via **Llama 3.2** (via Ollama).
    * **Frontend :** Interface Web interactive avec Streamlit (gestion de session et m√©moire).

---

## ‚öôÔ∏è Pr√©-requis

### 1. Syst√®me
* Python 3.10 ou sup√©rieur.
* [Ollama](https://ollama.com/) install√© et en cours d'ex√©cution.

### 2. Mod√®les Ollama
Vous devez t√©l√©charger les mod√®les suivants pour que le script fonctionne :

```bash
# Le "Cerveau" (LLM pour la g√©n√©ration)
ollama pull llama3.2

# La "M√©moire" (Mod√®le d'embedding pour la recherche vectorielle)
ollama pull nomic-embed-text
```

## üöÄ Installation

### 1. Cloner le projet

```Bash
git clone [https://github.com/votre-user/dev-rag-local.git](https://github.com/votre-user/dev-rag-local.git)
cd dev-rag-local
```

### 2. Cr√©er un environnement virtuel

```Bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Mac/Linux
source .venv/bin/activate
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```


Contenu du **requirements.txt** :


```Plaintext
langchain
langchain-ollama
langchain-chroma
langchain-community
beautifulsoup4
pypdf
streamlit
python-dotenv
```

### 4. Configuration (.env) Cr√©ez un fichier **.env** √† la racine pour d√©finir l'identit√© du scraper (√©vite les blocages 403 sur certains sites) :

Extrait de code

USER_AGENT="DevRAGLocal/1.0 (Student Project)"


#  üìñ Guide d'Utilisation

### √âtape 1 : Configurer les Sources

Ouvrez le fichier **build_db.py** et modifiez les variables :

PDFs : Placez vos fichiers dans le dossier **./mes_pdfs**.

URLs : Ajoutez les liens de documentation dans la liste **mes_urls**.

```python
mes_urls = [
    "[https://docs.python.org/fr/3/tutorial/index.html](https://docs.python.org/fr/3/tutorial/index.html)",
    "[https://fastapi.tiangolo.com/tutorial/](https://fastapi.tiangolo.com/tutorial/)",
]
```


### √âtape 2 : Construire la Base de Connaissances

Lancez le script d'ingestion. Il va scraper, indexer et vectoriser les donn√©es. Note : Gr√¢ce au syst√®me incr√©mentiel, relancer ce script ne traitera que les nouvelles URLs ou fichiers PDF.


```bash
python build_db.py
```

### √âtape 3 : Lancer l'Assistant

Option A : Interface Web (Recommand√©) Une interface chat moderne type "ChatGPT".

Installer streamlit

```bash
pip install streamlit
```
```bash
streamlit run interface.py
```

Option B : Terminal (CLI) Pour des tests rapides.

Bash
```bash
python main.py
```

# üìÇ Structure du Projet


```Plaintext
.
‚îú‚îÄ‚îÄ build_db.py           # Orchestrateur : Configure les sources et lance l'indexation
‚îú‚îÄ‚îÄ ingestion.py          # Moteur ETL : Scraping parall√®le, nettoyage HTML, gestion historique
‚îú‚îÄ‚îÄ main.py               # Backend : Logique RAG, connexion Ollama, classe DevChatBot
‚îú‚îÄ‚îÄ interface.py          # Frontend : Application Streamlit
‚îú‚îÄ‚îÄ mes_pdfs/             # Dossier pour vos fichiers PDF sources
‚îú‚îÄ‚îÄ raw_data_json/        # Sauvegarde interm√©diaire des donn√©es scrap√©es (Debug/Trace)
‚îú‚îÄ‚îÄ chroma_db_local/      # Base de donn√©es vectorielle (G√©n√©r√©e auto)
‚îú‚îÄ‚îÄ scraping_history.json # M√©moire du scraper (√©vite les doublons)
‚îî‚îÄ‚îÄ requirements.txt      # Liste des d√©pendances
```

# üõ†Ô∏è D√©pannage fr√©quent

Erreur **Connection refused** : Assurez-vous que l'application Ollama tourne en fond sur votre machine.

Scraping bloqu√© (TimeOut) : Le script utilise un User-Agent personnalis√©. V√©rifiez votre fichier **.env**. Certains sites (comme MDN) sont lents, le script a un timeout de 20s configur√©.

La base semble vide : V√©rifiez que **build_db.py** a bien affich√© "‚úÖ Base mise √† jour". Si vous changez de mod√®le d'embedding, supprimez le dossier **chroma_db_local** pour reconstruire proprement.